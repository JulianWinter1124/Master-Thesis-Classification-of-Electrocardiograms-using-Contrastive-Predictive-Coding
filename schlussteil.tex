\section{Discussion and Future work} \label{sec:discussion}

In a fully supervised setting, our baselines were able to produce slightly better prediction results compared to our Contrastive Predictive Coding models. Once we handicapped CPC by splitting the training into a pretraining and downstream training phase, with the same number of epochs given in total, the advance for the baseline models increased even further. Yet the CPC results were still above average.

Thus, when labels are available in large enough quantities, supervised learning methods, that have seen much research in the past, are probably easier to optimize and are able to produce better results as a consequence.

In environments where only few labels are given, which we tried to emulate in our "Low Label Availability" experiment in \autoref{section:lowlabel}, CPC showed better results compared to baseline models, when the least labels were given, after allowing sufficient training epochs. This is an indicator that CPC pretraining can improve classification results in fields where unlabeled data is readily available, while high quality labels are rare and only obtainable with high costs and effort --- which is the case in many fields of modern medicine.

Furthermore our augmented/noised data experiment in \autoref{sec:augmented} suggests that CPC pretraining increases training stability in the downstream task for unbalanced datasets, because both baseline- and the CPC networks that were trained fully supervised, mostly failed to classify the data satisfactory.

An additional strong benefit of CPC is time efficiency. Downstream tasks, where one has to train models with the same input data, but different labels, benefit of the shared  weights obtained during pretraining. Often CPC can yield good results with only few downstream epochs, reducing the necessary total training time drastically. This especially showed during training on the different train splits in "Low Label Availability" experiment: while the baselines were trained completely from ground up for every split, needing more than 24 hours wall clock time, the CPC models were trained in a fraction of that time.

CPC allows for many architectural choices, for example the encoder network and many hyperparameters like latent-size, context-size, number of summarized latents, number of predicted latents and the latent sampling method to name just a few. This makes it very flexible and applicable to many data modalities, as it does not make explicit demands on label shapes and does not require pseudo-labels like many other representation learning algorithms mentioned in the introduction, making it a strong contender for universal representation learning. This also shows in the original paper, where CPC was applied to multiple data modalities, such as images, timeseries or even reinforcement learning\autocite{DBLP:journals/corr/abs-1807-03748}. However the great flexibility might also bear the risk of finding only suboptimal network settings for your specific task. This can also be seen in our implementation \autoref{sec:additionalchanges}, where we showed that a context network is not even required to learn useful representations, but also that changing the encoder resulted in worse classification results, albeit giving more flexibility to the user.

Last but not least we applied grad-cam \autocite{DBLP:journals/corr/SelvarajuDVCPB16} to selected ECG samples, where a CPC network using our \code{cpc\_encoder\_likev8} encoder module, was voted the visually best performing network by a cardiologist.

In conclusion we infer that Contrastive Predictive Coding is an overall good method for classifying ECG-data, but for the pure classification task our baseline models that were trained purely supervised performed better and should be used instead. CPC networks' strengths can be seen when only few labels are available or the data is noisy, but we had to artificially create these environments for the ECG data to outperform the baseline models. CPC yielded the best results in our colorization task, which are however still far from perfect.

Future work might include the search for additional representation learning methods that also cope well with timeseries/sequential/non-image data. This includes the aforementioned altered CPC architectures, but also unrelated representation learning networks like VQ-VAE or more simple encoder-decoder architectures.

Additionally the findings of \autocite{song2020multilabelcontrastive} show that the original CPC has a high bias and needs a lot of negative samples to correctly approach the mutual information value. They introduce a revised CPC called $\alpha$-CPC that uses weights on both positive and negative samples. According to their formulation the original CPC then becomes a special case of $\alpha$-CPC. They also introduced a method called "Multi-label Contrastive Predictive Coding" where multiple positive samples are used to greatly decrease the theoretically needed amount of negative samples. Their conducted experiments showed a superior mutual information estimation and slightly better accuracy overall. We tried to reimplement $\alpha$-CPC but our networks failed to decrease the loss satisfactory. Correctly reproducing their findings could potentially lead to better results in our experiments. 
